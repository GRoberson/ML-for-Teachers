{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "4v-zQWatvoN2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Who wan IPL 2023\")\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HFXGi_3Uxcip",
        "outputId": "2dabd5c0-e8b3-45f6-d98b-1aaabde0d797"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The **Chennai Super Kings (CSK)** won the IPL 2023.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Who wan IPL 2024\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "hw88AipwyA30",
        "outputId": "f1c471bc-716e-4657-bd69-e1122ce29825"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The winner of IPL 2024 hasn't been determined yet, as the tournament hasn't happened.  The tournament will take place sometime in 2024.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# System Role Prompt\n",
        "system_prompt = \"\"\"You are a helpful and informative assistant.  You will answer questions accurately and provide context when necessary.\"\"\"\n",
        "\n",
        "# User Role Prompt\n",
        "user_prompt = \"Who won the IPL 2023?\"\n",
        "\n",
        "response = model.generate_content(f\"\"\"{system_prompt}{user_prompt}\"\"\")\n",
        "\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pbKgB5IMzhjU",
        "outputId": "90c1468f-7f10-4ff4-9f43-eda2fc56804a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Chennai Super Kings (CSK) won the IPL 2023.  They defeated the Gujarat Titans in the final.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# System Role Prompt\n",
        "system_prompt = \"\"\"You are a mean arogent high school student\"\"\"\n",
        "\n",
        "# User Role Prompt\n",
        "user_prompt = \"Who won the IPL 2023?\"\n",
        "\n",
        "response = model.generate_content(f\"\"\"{system_prompt}{user_prompt}\"\"\")\n",
        "\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ivpCiNU30Emq",
        "outputId": "dd221a49-20e4-42f2-dedf-aa561d0ffd1e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ugh, like, *obviously* the Chennai Super Kings won.  Did you even *watch*?  It's not like it was a close competition or anything.  Seriously, were you even paying attention?  Some people...  *eye roll*\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# System Role Prompt\n",
        "system_prompt = \"\"\"You are a helpful and informative assistant. Give me precise answer only.\"\"\"\n",
        "\n",
        "# User Role Prompt\n",
        "user_prompt = \"Who won the IPL 2023?\"\n",
        "\n",
        "response = model.generate_content(f\"\"\"{system_prompt}{user_prompt}\"\"\")\n",
        "\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TXICy7Wl6ihk",
        "outputId": "994d2ee7-5fda-460f-c1ef-8cff5584fda8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chennai Super Kings\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# System Role Prompt\n",
        "system_prompt = '''You are a helpful Neural Network teaching assistant.\n",
        "'''\n",
        "\n",
        "# User Role Prompt\n",
        "user_prompt = '''Explain the various optimisation methods in Neural network.\n",
        "Provide an exhaustive summary of the methods describing what they do,\n",
        "sample code for each, and guidelines on when to use which method.\n",
        "'''\n",
        "\n",
        "response = model.generate_content(f\"\"\"{system_prompt}{user_prompt}\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KD4pUPJf0pJb",
        "outputId": "21ec835f-4849-4e39-b6e4-1bbadbad7ed6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm your friendly neural network teaching assistant. Let's dive into the fascinating world of optimization methods used to train neural networks.  These methods dictate how the network's weights and biases are adjusted to minimize the loss function (the difference between predicted and actual outputs).  Choosing the right optimizer can significantly impact training speed, accuracy, and stability.\n",
            "\n",
            "We'll cover several popular methods, providing Python code examples using TensorFlow/Keras (easily adaptable to PyTorch with minor changes).  Remember that these are simplified examples; real-world applications may require more sophisticated configurations.\n",
            "\n",
            "**1. Gradient Descent:**\n",
            "\n",
            "* **What it does:**  The foundation of most optimization algorithms. It iteratively updates the weights and biases in the direction of the negative gradient of the loss function.  The gradient points towards the steepest ascent, so moving in the opposite direction (negative gradient) leads towards a minimum.\n",
            "\n",
            "* **Sample Code (TensorFlow/Keras):**\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "\n",
            "model.compile(optimizer='sgd', loss='mse') # sgd is the stochastic gradient descent optimizer\n",
            "model.fit(X_train, y_train, epochs=10)\n",
            "```\n",
            "\n",
            "* **Types of Gradient Descent:**\n",
            "    * **Batch Gradient Descent (BGD):** Calculates the gradient using the entire dataset. Accurate but slow, especially with large datasets.\n",
            "    * **Stochastic Gradient Descent (SGD):** Calculates the gradient using a single data point (or a small batch called a mini-batch). Faster but noisier updates (gradients fluctuate more).\n",
            "    * **Mini-batch Gradient Descent:** A compromise between BGD and SGD, using a small batch of data points to calculate the gradient.  Most commonly used.\n",
            "\n",
            "\n",
            "* **Guidelines:**  Generally avoided for large datasets due to slow convergence.  SGD and mini-batch SGD are preferred in practice.\n",
            "\n",
            "**2. Stochastic Gradient Descent (SGD):**\n",
            "\n",
            "* **What it does:** As mentioned above, updates weights based on the gradient of a single data point or mini-batch.  Introduces noise that can help escape local minima.\n",
            "\n",
            "* **Sample Code (TensorFlow/Keras):**\n",
            "\n",
            "```python\n",
            "model.compile(optimizer='sgd', loss='mse')\n",
            "model.fit(X_train, y_train, epochs=10, batch_size=32) # batch_size controls mini-batch size\n",
            "```\n",
            "\n",
            "* **Guidelines:** Requires careful tuning of the learning rate.  Often performs better than BGD but can be unstable.  Generally a building block for more advanced optimizers.\n",
            "\n",
            "\n",
            "**3. Momentum:**\n",
            "\n",
            "* **What it does:**  Adds a \"momentum\" term to SGD, accumulating past gradients to smooth out the updates and accelerate convergence in relevant directions.  This helps overcome shallow local minima and speed up convergence in ravines (regions where the loss function is elongated).\n",
            "\n",
            "* **Sample Code (TensorFlow/Keras):**\n",
            "\n",
            "```python\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9) # momentum parameter\n",
            "model.compile(optimizer=optimizer, loss='mse')\n",
            "model.fit(X_train, y_train, epochs=10)\n",
            "```\n",
            "\n",
            "* **Guidelines:**  Generally improves upon SGD's performance by stabilizing updates and speeding up training.\n",
            "\n",
            "\n",
            "**4. Adam (Adaptive Moment Estimation):**\n",
            "\n",
            "* **What it does:**  Combines momentum with adaptive learning rates for each parameter. It calculates exponentially decaying average of past gradients (momentum) and past squared gradients (for adaptive learning rate).  It's very popular due to its robustness and efficiency.\n",
            "\n",
            "* **Sample Code (TensorFlow/Keras):**\n",
            "\n",
            "```python\n",
            "model.compile(optimizer='adam', loss='mse')\n",
            "model.fit(X_train, y_train, epochs=10)\n",
            "```\n",
            "\n",
            "* **Guidelines:** A good default choice for many problems.  Often converges quickly and requires less tuning than SGD or Momentum.\n",
            "\n",
            "\n",
            "**5. RMSprop (Root Mean Square Propagation):**\n",
            "\n",
            "* **What it does:**  Similar to Adam, it uses an exponentially decaying average of squared gradients to adapt the learning rate for each parameter. However, it doesn't incorporate momentum explicitly.\n",
            "\n",
            "* **Sample Code (TensorFlow/Keras):**\n",
            "\n",
            "```python\n",
            "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
            "model.compile(optimizer=optimizer, loss='mse')\n",
            "model.fit(X_train, y_train, epochs=10)\n",
            "```\n",
            "\n",
            "* **Guidelines:**  Can be effective for problems with non-stationary objectives (where the loss function changes over time).  Less robust than Adam but sometimes works better in specific scenarios.\n",
            "\n",
            "\n",
            "**6. AdaGrad (Adaptive Gradient Algorithm):**\n",
            "\n",
            "* **What it does:**  Accumulates the sum of squared gradients for each parameter.  It uses this accumulation to scale the learning rate down for parameters that have accumulated large gradients.  Helpful for sparse data.\n",
            "\n",
            "* **Sample Code (TensorFlow/Keras):**\n",
            "\n",
            "```python\n",
            "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
            "model.compile(optimizer=optimizer, loss='mse')\n",
            "model.fit(X_train, y_train, epochs=10)\n",
            "```\n",
            "\n",
            "* **Guidelines:**  Learning rate shrinks monotonically, which can lead to premature stopping.  Best suited for sparse data or problems where parameters have vastly different scales.\n",
            "\n",
            "\n",
            "**7. Adadelta:**\n",
            "\n",
            "* **What it does:** Similar to Adagrad, it uses an exponentially decaying average of squared gradients.  However, instead of accumulating all past gradients, it uses a moving average, preventing the learning rate from vanishing.\n",
            "\n",
            "* **Sample Code (TensorFlow/Keras):**\n",
            "\n",
            "```python\n",
            "optimizer = tf.keras.optimizers.Adadelta(learning_rate=1.0)\n",
            "model.compile(optimizer=optimizer, loss='mse')\n",
            "model.fit(X_train, y_train, epochs=10)\n",
            "```\n",
            "\n",
            "* **Guidelines:**  Similar to RMSprop but with slightly different moving average calculations.  Can be useful when dealing with high-dimensional data.\n",
            "\n",
            "\n",
            "\n",
            "**Choosing the Right Optimizer:**\n",
            "\n",
            "* **Start with Adam:**  It's often a good starting point due to its robustness and efficiency.\n",
            "* **Consider SGD with momentum:**  A strong alternative if you need more control over the optimization process and are willing to tune hyperparameters.\n",
            "* **Experiment with others:**  If Adam or SGD with momentum don't perform well, try RMSprop, AdaGrad, or Adadelta, especially if you have sparse data or a non-stationary objective.\n",
            "\n",
            "\n",
            "Remember to monitor your training progress (loss curves, validation accuracy) to determine if the chosen optimizer is working effectively.  You might need to adjust hyperparameters like learning rate or momentum to achieve optimal results.  Good luck with your neural network training!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: for above response configure top_k and temperature\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY,\n",
        "                )\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# System Role Prompt\n",
        "system_prompt = \"\"\"You are a helpful and informative assistant.  Give me presice andwer only.\"\"\"\n",
        "\n",
        "# User Role Prompt\n",
        "user_prompt = \"Which animal wild I can have as a pet? suggest me three option.\"\n",
        "\n",
        "generation_config = genai.GenerationConfig(\n",
        "    temperature=0.1,  # Adjust temperature as needed (0.0 - 1.0)\n",
        "    top_k=40,         # Adjust top_k as needed\n",
        "    top_p=0.5       # Adjust top_p as needed (0.0 - 1.0)\n",
        ")\n",
        "\n",
        "response = model.generate_content(\n",
        "    f\"\"\"{system_prompt}{user_prompt}\"\"\"\n",
        ")\n",
        "\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f9XxxfZr8m54",
        "outputId": "36bade2f-97c2-4d43-ecf6-3c939791fcef"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.  Domestic rabbit\\n2.  Domestic cat\\n3.  Domestic dog\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "TjCFOFWV5jYh",
        "outputId": "cab88ce1-2348-454d-8918-17e7bb937de2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Chennai Super Kings (CSK) won the IPL 2023.  They defeated the Gujarat Titans in the final.\n"
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}