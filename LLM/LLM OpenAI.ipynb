{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mBdCgR9efxf_",
        "outputId": "0bac9062-bdd3-41ec-ca72-2d8371a990b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "rHT4bfzngDrk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won IPL 2020\"}\n",
        "    ]"
      ],
      "metadata": {
        "id": "ysVSvh8sgj0I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# openai.api_key = \"hadjkshfkjdafk jdhfkjadhfkjadhkf\"\n",
        "from google.colab import userdata # import to use Colab Credential\n",
        "\n",
        "openai.api_key=userdata.get('OPENAI_API_KEY') # Get your API Key"
      ],
      "metadata": {
        "id": "gEv-xgJvgkNc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages\n",
        ")"
      ],
      "metadata": {
        "id": "vwr2g8stgnwt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GpxFnmYfhJQA",
        "outputId": "984a8d69-7319-4edc-c6b7-c8539262128b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-AbliZ3ClyBvpdmBtCe5gqknrayizB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Mumbai Indians won the Indian Premier League (IPL) in 2020.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733564927, model='gpt-3.5-turbo-16k-0613', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=23, total_tokens=40, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OJ_wpVgXhLmU",
        "outputId": "cdbb9a3a-4e23-4a26-8e5d-0add470de4bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Mumbai Indians won the Indian Premier League (IPL) in 2020.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "    {\"role\": \"user\", \"content\": \"You are a very helpfull assistant. Who won IPL 2020\"}\n",
        "    ]"
      ],
      "metadata": {
        "id": "pCS_3vFvhRLL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages\n",
        ")\n",
        "chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UOn585yOheMA",
        "outputId": "1b3b3eab-72cf-40d0-fbba-36de0112d72a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mumbai Indians won IPL 2020.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''You are a helpful Neural Network teaching assistant.\n",
        "Explain the various optimisation methods in Neural network.\n",
        "Provide an exhaustive summary of the methods describing what they do,\n",
        "sample code for each, and guidelines on when to use which method.\n",
        "'''\n",
        "\n",
        "message = [{\"role\": \"user\", \"content\": prompt}]"
      ],
      "metadata": {
        "id": "6WpJoVZwhhSR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=message,\n",
        "    max_tokens=800,\n",
        "    temperature=0.5,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    top_p = 0.6,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0)\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrKEEomKhvzF",
        "outputId": "8de7c64e-28a9-4b7e-a0a3-f6fd8e4e7800"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are several optimization methods commonly used in neural networks to train the model efficiently. These methods help in finding the optimal set of weights and biases that minimize the loss function. Here is an exhaustive summary of some popular optimization methods along with sample code and guidelines on when to use each method:\n",
            "\n",
            "1. Gradient Descent:\n",
            "   - Gradient descent is a basic optimization algorithm that updates the weights in the opposite direction of the gradient of the loss function.\n",
            "   - It has different variants such as batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.\n",
            "   - Sample code:\n",
            "     ```python\n",
            "     learning_rate = 0.01\n",
            "     for epoch in range(num_epochs):\n",
            "         # Compute gradients\n",
            "         gradients = compute_gradients(loss_function)\n",
            "         # Update weights\n",
            "         weights -= learning_rate * gradients\n",
            "     ```\n",
            "   - Use gradient descent when the dataset fits in memory and the number of samples is not too large.\n",
            "\n",
            "2. Momentum:\n",
            "   - Momentum is an optimization method that adds a fraction of the previous weight update to the current update.\n",
            "   - It helps accelerate convergence and overcome local minima.\n",
            "   - Sample code:\n",
            "     ```python\n",
            "     learning_rate = 0.01\n",
            "     momentum = 0.9\n",
            "     velocity = 0\n",
            "     for epoch in range(num_epochs):\n",
            "         # Compute gradients\n",
            "         gradients = compute_gradients(loss_function)\n",
            "         # Update velocity\n",
            "         velocity = momentum * velocity - learning_rate * gradients\n",
            "         # Update weights\n",
            "         weights += velocity\n",
            "     ```\n",
            "   - Use momentum when the loss function has plateaus or noisy gradients.\n",
            "\n",
            "3. AdaGrad:\n",
            "   - AdaGrad adapts the learning rate for each parameter based on the historical gradients.\n",
            "   - It reduces the learning rate for frequently occurring features and increases it for infrequent features.\n",
            "   - Sample code:\n",
            "     ```python\n",
            "     learning_rate = 0.01\n",
            "     epsilon = 1e-8\n",
            "     cache = 0\n",
            "     for epoch in range(num_epochs):\n",
            "         # Compute gradients\n",
            "         gradients = compute_gradients(loss_function)\n",
            "         # Update cache\n",
            "         cache += gradients ** 2\n",
            "         # Update weights\n",
            "         weights -= learning_rate * gradients / (np.sqrt(cache) + epsilon)\n",
            "     ```\n",
            "   - Use AdaGrad when dealing with sparse data or when the learning rate needs to be adapted automatically.\n",
            "\n",
            "4. RMSprop:\n",
            "   - RMSprop is an optimization method that addresses the diminishing learning rate problem of AdaGrad.\n",
            "   - It uses an exponentially weighted moving average of squared gradients to normalize the learning rate.\n",
            "   - Sample code:\n",
            "     ```python\n",
            "     learning_rate = 0.01\n",
            "     decay_rate = 0.9\n",
            "     epsilon = 1e-8\n",
            "     cache = 0\n",
            "     for epoch in range(num_epochs):\n",
            "         # Compute gradients\n",
            "         gradients = compute_gradients(loss_function)\n",
            "         # Update cache\n",
            "         cache = decay_rate * cache + (1 - decay_rate) * gradients ** 2\n",
            "         # Update weights\n",
            "         weights -= learning_rate * gradients / (np.sqrt(cache) + epsilon)\n",
            "     ```\n",
            "   - Use RMSprop when dealing with non-stationary objectives or when AdaGrad's learning rate becomes too small.\n",
            "\n",
            "5. Adam:\n",
            "   - Adam combines the benefits of momentum and RMSprop by using both first and second-order moments of the gradients.\n",
            "   - It adapts the learning rate for each parameter and provides bias correction.\n",
            "   - Sample code:\n",
            "     ```python\n",
            "     learning_rate = 0.01\n",
            "     beta1 = 0.9\n",
            "     beta2 = 0.999\n",
            "     epsilon = 1e-8\n",
            "     m = 0\n",
            "     v = 0\n",
            "     t = 0\n",
            "     for epoch in range(num_epochs):\n",
            "         # Compute gradients\n",
            "         gradients = compute_gradients(loss_function)\n",
            "         # Update moments\n",
            "         t += 1\n",
            "         m = beta1 * m + (1 - beta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ya5ejQ7Zh0DT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}